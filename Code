"""
Agent-of-Agents: Production-Ready Python Scaffold v0.1
======================================================
A minimal, stdlib-only framework for defining and executing AI agents
with validation, QA metrics, and mock tools.

Features:
- JSON-based agent configuration with comprehensive validation
- Bounded execution loop with configurable constraints
- Quality assurance metrics (confidence, consistency, rule compliance)
- Mock tool implementations (replace with real integrations)
- Clear error messages and logging

Usage:
    python agent_scaffold.py [--form FORM] [--input INPUT] [--out OUT]

Examples:
    python agent_scaffold.py
    python agent_scaffold.py --form my_agent.json --input params.json --out results

Requirements: Python 3.8+, no external dependencies
"""
from __future__ import annotations

import argparse
import dataclasses
import json
import logging
import random
import re
import sys
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

__version__ = "0.1.0"

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)


# ============================================================================
# Data Models
# ============================================================================

@dataclass
class AgentForm:
    """User-facing agent configuration form.
    
    This structure accepts simple, human-readable keys. Missing fields
    receive sensible defaults during compilation to AgentSpec.
    """
    name: Optional[str] = None
    goal: Optional[str] = None
    focus: Optional[str] = None
    display: Optional[Dict[str, Any]] = None
    inputs: Optional[str] = None
    tools: Optional[str] = None
    specifics: Optional[Dict[str, Any]] = None
    approvals: Optional[str] = None
    memory: Optional[str] = None
    complexity: Optional[str] = None
    autonomy: Optional[str] = None
    notes: Optional[str] = None


@dataclass
class AgentSpec:
    """Machine-readable agent specification.
    
    Compiled from AgentForm with all defaults resolved and structure
    normalized for execution.
    """
    name: str
    goal: str
    tags: List[str] = field(default_factory=list)
    inputs: List[str] = field(default_factory=list)
    outputs: List[str] = field(default_factory=list)
    display_addons: List[str] = field(default_factory=list)
    tools: List[str] = field(default_factory=list)
    plan: Dict[str, List[str]] = field(default_factory=dict)
    constraints: Dict[str, Any] = field(default_factory=dict)
    approvals: List[str] = field(default_factory=list)
    memory: Dict[str, bool] = field(default_factory=dict)
    control: Dict[str, Any] = field(default_factory=dict)
    qa: Dict[str, Any] = field(default_factory=dict)
    notes: str = ""


# ============================================================================
# Validation
# ============================================================================

class ValidationError(Exception):
    """Raised when agent form validation fails."""
    def __init__(self, errors: List[str]):
        self.errors = errors
        super().__init__(f"Validation failed with {len(errors)} error(s)")


class Validator:
    """Validates AgentForm configurations against business rules."""
    
    TOOL_CATALOG = {
        "search", "fetch", "summarize", "write", "classify", "extract",
        "browser", "file_read", "file_write", "codegen"
    }
    
    DISPLAY_TYPES = {"bullet-points", "summary", "report"}
    
    MEMORY_MODES = {"short", "long", "both", "none"}
    
    LEVELS = {"low", "medium", "high"}
    
    ACTION_VERBS = {
        "prepare", "produce", "generate", "find", "summarize", "organize",
        "draft", "plan", "create", "build", "analyze", "extract", "compile"
    }
    
    @classmethod
    def validate_form(cls, form: AgentForm) -> Tuple[bool, List[str]]:
        """Validate agent form and return (success, errors).
        
        Args:
            form: The agent form to validate
            
        Returns:
            Tuple of (is_valid, list_of_error_messages)
        """
        errors: List[str] = []
        
        # Name validation
        if not form.name:
            errors.append("Name is required (2-40 characters).")
        elif not isinstance(form.name, str):
            errors.append("Name must be a string.")
        elif not re.fullmatch(r"[A-Za-z0-9_-]{2,40}", form.name):
            errors.append(
                "Name must be 2-40 characters, containing only letters, "
                "numbers, dashes, or underscores."
            )
        
        # Goal validation
        if not form.goal:
            errors.append("Goal is required (1-240 characters).")
        elif not isinstance(form.goal, str):
            errors.append("Goal must be a string.")
        elif not (1 <= len(form.goal) <= 240):
            errors.append(f"Goal must be 1-240 characters (got {len(form.goal)}).")
        elif not any(verb in form.goal.lower() for verb in cls.ACTION_VERBS):
            errors.append(
                f"Goal should contain an action verb (e.g., {', '.join(sorted(cls.ACTION_VERBS)[:5])})."
            )
        
        # Display validation
        if form.display is not None:
            if not isinstance(form.display, dict):
                errors.append("Display must be a dictionary/object.")
            else:
                dtype = form.display.get("Type", "").strip()
                if dtype and dtype not in cls.DISPLAY_TYPES:
                    errors.append(
                        f"Display.Type must be one of: {', '.join(sorted(cls.DISPLAY_TYPES))}."
                    )
        
        # Tools validation
        if form.tools:
            if not isinstance(form.tools, str):
                errors.append("Tools must be a comma-separated string.")
            else:
                tool_list = [t.strip() for t in form.tools.split(',') if t.strip()]
                for tool in tool_list:
                    if tool not in cls.TOOL_CATALOG:
                        errors.append(
                            f"Unknown tool '{tool}'. Available: {', '.join(sorted(cls.TOOL_CATALOG))}"
                        )
        
        # Memory validation
        if form.memory and form.memory not in cls.MEMORY_MODES:
            errors.append(
                f"Memory must be one of: {', '.join(sorted(cls.MEMORY_MODES))}."
            )
        
        # Complexity validation
        if form.complexity and form.complexity not in cls.LEVELS:
            errors.append(
                f"Complexity must be one of: {', '.join(sorted(cls.LEVELS))}."
            )
        
        # Autonomy validation
        if form.autonomy and form.autonomy not in cls.LEVELS:
            errors.append(
                f"Autonomy must be one of: {', '.join(sorted(cls.LEVELS))}."
            )
        
        # Specifics validation
        if form.specifics is not None and not isinstance(form.specifics, dict):
            errors.append("Specifics must be a dictionary/object.")
        
        return (len(errors) == 0, errors)


# ============================================================================
# Compilation
# ============================================================================

class Compiler:
    """Compiles user-friendly AgentForm into machine-readable AgentSpec."""
    
    DISPLAY_TO_OUTPUT = {
        "bullet-points": ["brief.md"],
        "summary": ["summary.md"],
        "report": ["report.md"],
    }
    
    @staticmethod
    def _split_csv(s: Optional[str]) -> List[str]:
        """Split comma-separated values, stripping whitespace."""
        if not s or not isinstance(s, str):
            return []
        return [x.strip() for x in s.split(',') if x.strip()]
    
    @staticmethod
    def _bools_from_memory(mem: Optional[str]) -> Dict[str, bool]:
        """Convert memory mode string to boolean flags."""
        mem = (mem or "short").strip().lower()
        return {
            "short_term": mem in {"short", "both"},
            "long_term": mem in {"long", "both"},
        }
    
    @staticmethod
    def _constraints_from_complexity(cplx: Optional[str]) -> Tuple[int, str]:
        """Map complexity level to execution constraints."""
        cplx = (cplx or "medium").strip().lower()
        if cplx == "low":
            return 4, "shallow"
        if cplx == "high":
            return 12, "deep"
        return 8, "standard"
    
    @staticmethod
    def _human_gates_from_autonomy(auto: Optional[str]) -> List[str]:
        """Map autonomy level to required human approval points."""
        auto = (auto or "medium").strip().lower()
        if auto == "low":
            return ["plan", "final"]
        if auto == "high":
            return []
        return ["final"]
    
    @classmethod
    def compile(cls, form: AgentForm) -> AgentSpec:
        """Compile AgentForm into AgentSpec with all defaults resolved.
        
        Args:
            form: Validated agent form
            
        Returns:
            Complete agent specification ready for execution
        """
        # Extract display configuration
        display_type = "summary"
        addons: List[str] = []
        if isinstance(form.display, dict):
            display_type = form.display.get("Type", "").strip() or "summary"
            addons = cls._split_csv(form.display.get("Add-ons"))
        
        outputs = cls.DISPLAY_TO_OUTPUT.get(display_type, ["summary.md"])
        inputs = cls._split_csv(form.inputs) or ["topic"]
        tools = cls._split_csv(form.tools) or ["search", "fetch", "summarize", "write"]
        tags = cls._split_csv(form.focus)
        approvals = cls._split_csv(form.approvals)
        
        # Extract workflow steps and domain constraints
        steps: List[str] = []
        whitelist: List[str] = []
        blacklist: List[str] = []
        
        if isinstance(form.specifics, dict):
            steps_text = form.specifics.get("Steps", "")
            if steps_text:
                # Split on arrows or commas
                parts = re.split(r"→|->|,", steps_text)
                steps = [p.strip() for p in parts if p.strip()]
            
            domains = form.specifics.get("Domains", {})
            if isinstance(domains, dict):
                whitelist = cls._split_csv(domains.get("whitelist", ""))
                blacklist = cls._split_csv(domains.get("blacklist", ""))
        
        # Derive control parameters
        max_steps, depth = cls._constraints_from_complexity(form.complexity)
        human_gates = cls._human_gates_from_autonomy(form.autonomy)
        memory = cls._bools_from_memory(form.memory)
        
        # QA configuration
        qa = {
            "enable_confidence": True,
            "checklist": [
                "plan_exists",
                "steps_coherent",
                "constraints_applied",
                "failures_handled"
            ],
            "metrics": [
                "p_action_success",
                "self_consistency",
                "rule_checks"
            ],
        }
        
        # Build constraints
        constraints = {
            "domains_whitelist": whitelist,
            "domains_blacklist": blacklist,
            "max_steps": max_steps,
            "depth": depth,
        }
        
        # Extract timeframe hint from notes
        if form.notes:
            match = re.search(
                r"last\s+([0-9]+)\s*(days?|weeks?|months?|years?)",
                form.notes,
                flags=re.I
            )
            if match:
                constraints["timeframe_hint"] = f"{match.group(1)} {match.group(2)}"
        
        return AgentSpec(
            name=form.name or "agent",
            goal=form.goal or "",
            tags=tags,
            inputs=inputs,
            outputs=outputs,
            display_addons=addons,
            tools=tools,
            plan={"hints": steps},
            constraints=constraints,
            approvals=approvals,
            memory=memory,
            control={"human_gates": human_gates},
            qa=qa,
            notes=form.notes or "",
        )


# ============================================================================
# Tool System
# ============================================================================

class ToolContext:
    """Execution context shared across tool invocations."""
    
    def __init__(self, spec: AgentSpec, run_input: Dict[str, Any], out_dir: Path):
        self.spec = spec
        self.run_input = run_input
        self.out_dir = out_dir
        self.logs: List[str] = []
        self.sources: List[Dict[str, Any]] = []
        self._start_time = time.time()
    
    def log(self, msg: str, level: str = "INFO") -> None:
        """Add timestamped log entry."""
        timestamp = time.strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {msg}"
        self.logs.append(log_entry)
        
        if level == "ERROR":
            logger.error(msg)
        elif level == "WARNING":
            logger.warning(msg)
        else:
            logger.info(msg)
    
    def elapsed(self) -> float:
        """Return elapsed time in seconds since context creation."""
        return time.time() - self._start_time


class Tool:
    """Base class for agent tools."""
    name: str = "base_tool"
    
    def __call__(self, ctx: ToolContext, **kwargs) -> Any:
        raise NotImplementedError(f"Tool {self.name} not implemented")


class SearchTool(Tool):
    """Mock search tool - simulates web search results."""
    name = "search"
    
    def __call__(self, ctx: ToolContext, query: str = "") -> List[Dict[str, Any]]:
        """Simulate search results for a query.
        
        Args:
            ctx: Tool execution context
            query: Search query string
            
        Returns:
            List of simulated search results with URLs and metadata
        """
        topic = query or ctx.run_input.get("topic", "topic")
        ctx.log(f"search: '{topic}' → simulating 6 results")
        
        # Use deterministic randomness based on topic
        rnd = random.Random(hash(topic) & 0xFFFFFFFF)
        domains = ["example.gov", "example.edu", "news.example.com", "research.example.org"]
        
        results = []
        for i in range(6):
            domain = rnd.choice(domains)
            timestamp = int(time.time()) - rnd.randint(0, 60 * 60 * 24 * 730)  # 0-2 years
            
            results.append({
                "title": f"{topic.title()} - Source {i + 1}",
                "url": f"https://{domain}/{topic.replace(' ', '-').lower()}/{i + 1}",
                "timestamp": timestamp,
                "domain": domain,
                "snippet": f"Relevant information about {topic}...",
            })
        
        ctx.sources.extend(results)
        return results


class FetchTool(Tool):
    """Mock fetch tool - simulates fetching web page content."""
    name = "fetch"
    
    def __call__(self, ctx: ToolContext, urls: List[str]) -> List[Dict[str, Any]]:
        """Simulate fetching content from URLs.
        
        Args:
            ctx: Tool execution context
            urls: List of URLs to fetch
            
        Returns:
            List of documents with URL and simulated content
        """
        if not urls:
            ctx.log("fetch: no URLs provided", level="WARNING")
            return []
        
        ctx.log(f"fetch: retrieving {len(urls)} URL(s)")
        
        docs = []
        for url in urls[:10]:  # Limit to 10 for safety
            docs.append({
                "url": url,
                "text": f"[Simulated content from {url}]\n\nThis is mock content for testing purposes.",
                "status": 200,
                "timestamp": int(time.time()),
            })
        
        return docs


class SummarizeTool(Tool):
    """Mock summarize tool - simulates content summarization."""
    name = "summarize"
    
    def __call__(self, ctx: ToolContext, docs: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        """Simulate summarizing documents into themes.
        
        Args:
            ctx: Tool execution context
            docs: List of documents to summarize
            
        Returns:
            Dictionary mapping theme names to key points
        """
        if not docs:
            ctx.log("summarize: no documents provided", level="WARNING")
            return {}
        
        topic = ctx.run_input.get("topic", "topic")
        # Extract last meaningful word for variation
        topic_word = topic.split()[-1] if topic else "topic"
        
        ctx.log(f"summarize: analyzing {len(docs)} document(s) → extracting themes")
        
        # Generate simulated themes
        themes = {
            f"Theme: {topic_word.title()} Integrity": [
                "Source verification methods",
                "Quality assessment criteria",
                "Reliability indicators"
            ],
            f"Theme: {topic_word.title()} Challenges": [
                "Common obstacles identified",
                "Risk factors and vulnerabilities",
                "Mitigation strategies"
            ],
            f"Theme: {topic_word.title()} Solutions": [
                "Best practices recommended",
                "Technical implementations",
                "Policy frameworks"
            ],
        }
        
        return themes


class WriteTool(Tool):
    """Mock write tool - generates markdown output."""
    name = "write"
    
    def __call__(self, ctx: ToolContext, bullets: List[Dict[str, Any]]) -> Path:
        """Write bullet points to markdown file.
        
        Args:
            ctx: Tool execution context
            bullets: List of bullet point dictionaries with title, body, etc.
            
        Returns:
            Path to written output file
        """
        if not bullets:
            ctx.log("write: no content provided", level="WARNING")
        
        output_filename = ctx.spec.outputs[0] if ctx.spec.outputs else "output.md"
        output_path = ctx.out_dir / output_filename
        
        # Build markdown content
        title = output_filename.replace(".md", "").replace("_", " ").title()
        lines = [f"# {title}\n\n"]
        lines.append(f"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        if ctx.spec.goal:
            lines.append(f"**Goal:** {ctx.spec.goal}\n\n")
        
        lines.append("---\n\n")
        
        for bullet in bullets:
            title_text = bullet.get("title", "Untitled")
            body_text = bullet.get("body", "")
            confidence = bullet.get("confidence")
            sources = bullet.get("sources", [])
            
            lines.append(f"## {title_text}\n\n")
            lines.append(f"{body_text}\n\n")
            
            if confidence is not None:
                lines.append(f"*Confidence: {confidence:.2%}*\n\n")
            
            if sources:
                lines.append("**Sources:**\n\n")
                for src in sources[:3]:  # Limit to 3 sources per bullet
                    src_url = src.get("url", "")
                    src_title = src.get("title", src_url)
                    lines.append(f"- [{src_title}]({src_url})\n")
                lines.append("\n")
        
        # Write file
        try:
            output_path.write_text("".join(lines), encoding="utf-8")
            ctx.log(f"write: created {output_path}")
        except IOError as e:
            ctx.log(f"write: failed to write {output_path}: {e}", level="ERROR")
            raise
        
        # Write sources manifest
        try:
            sources_path = ctx.out_dir / "sources.json"
            sources_path.write_text(
                json.dumps(ctx.sources, indent=2, ensure_ascii=False),
                encoding="utf-8"
            )
            ctx.log(f"write: created {sources_path}")
        except IOError as e:
            ctx.log(f"write: failed to write sources: {e}", level="WARNING")
        
        return output_path


# ============================================================================
# Quality Assurance
# ============================================================================

class QA:
    """Quality assurance metrics for agent execution."""
    
    @staticmethod
    def probability_of_action_success(history: List[str]) -> float:
        """Estimate probability that actions succeeded based on logs.
        
        Args:
            history: List of log messages
            
        Returns:
            Probability score between 0.0 and 1.0
        """
        if not history:
            return 0.5
        
        success_indicators = ["simulated", "wrote", "created", "→", "docs", "results"]
        error_indicators = ["error", "failed", "warning"]
        
        success_count = sum(
            1 for log in history
            if any(indicator in log.lower() for indicator in success_indicators)
        )
        error_count = sum(
            1 for log in history
            if any(indicator in log.lower() for indicator in error_indicators)
        )
        
        # Base score plus success bonus minus error penalty
        score = 0.5 + (success_count / max(1, len(history))) * 0.4
        score -= (error_count / max(1, len(history))) * 0.3
        
        return max(0.0, min(1.0, score))
    
    @staticmethod
    def self_consistency_passes(texts: List[str]) -> float:
        """Measure consistency between text outputs using token overlap.
        
        Args:
            texts: List of text strings to compare
            
        Returns:
            Consistency score between 0.0 and 1.0
        """
        if len(texts) < 2:
            return 1.0
        
        # Compare first two texts using Jaccard similarity
        tokens_a = set(texts[0].lower().split())
        tokens_b = set(texts[1].lower().split())
        
        if not tokens_a and not tokens_b:
            return 1.0
        
        intersection = len(tokens_a & tokens_b)
        union = len(tokens_a | tokens_b)
        
        if union == 0:
            return 0.0
        
        jaccard = intersection / union
        
        # Scale to 0.6-1.0 range (partial overlap is expected)
        return 0.6 + 0.4 * jaccard
    
    @staticmethod
    def rule_checks(spec: AgentSpec, ctx: ToolContext) -> float:
        """Check compliance with domain whitelist/blacklist rules.
        
        Args:
            spec: Agent specification with constraints
            ctx: Execution context with sources
            
        Returns:
            Compliance score between 0.0 and 1.0
        """
        whitelist = spec.constraints.get("domains_whitelist", [])
        blacklist = spec.constraints.get("domains_blacklist", [])
        
        if not whitelist and not blacklist:
            return 0.8  # No rules to check
        
        if not ctx.sources:
            return 0.5  # No sources to validate
        
        score = 0.7
        violations = 0
        
        for source in ctx.sources[:10]:  # Check first 10 sources
            domain = source.get("domain", "").lower()
            
            # Check whitelist compliance
            if whitelist:
                if any(wl.replace("*.", "").lower() in domain for wl in whitelist):
                    score += 0.03
                else:
                    violations += 1
            
            # Check blacklist violations
            if blacklist:
                if any(bl.replace("*.", "").lower() in domain for bl in blacklist):
                    violations += 1
                    score -= 0.05
        
        # Penalty for violations
        if violations > 0:
            score -= violations * 0.1
        
        return max(0.0, min(1.0, score))
    
    @classmethod
    def compute_metrics(cls, spec: AgentSpec, ctx: ToolContext, 
                       outputs: List[str]) -> Dict[str, float]:
        """Compute all QA metrics for an execution.
        
        Args:
            spec: Agent specification
            ctx: Execution context
            outputs: List of output text strings
            
        Returns:
            Dictionary of metric name to score
        """
        p_action = cls.probability_of_action_success(ctx.logs)
        consistency = cls.self_consistency_passes(outputs[:2])
        rules = cls.rule_checks(spec, ctx)
        
        # Chain accuracy: weighted combination
        chain_accuracy = (
            0.4 * p_action +
            0.3 * consistency +
            0.3 * rules
        )
        
        return {
            "p_action_success": round(p_action, 3),
            "self_consistency": round(consistency, 3),
            "rule_checks": round(rules, 3),
            "chain_accuracy": round(chain_accuracy, 3),
        }


# ============================================================================
# Orchestration
# ============================================================================

class Orchestrator:
    """Orchestrates agent execution with tools and QA."""
    
    def __init__(self, spec: AgentSpec, out_dir: Path):
        """Initialize orchestrator.
        
        Args:
            spec: Agent specification
            out_dir: Output directory for artifacts
        """
        self.spec = spec
        self.out_dir = out_dir
        self.ctx = ToolContext(spec, {}, out_dir)
        
        # Tool registry
        self.tools: Dict[str, Tool] = {
            "search": SearchTool(),
            "fetch": FetchTool(),
            "summarize": SummarizeTool(),
            "write": WriteTool(),
        }
    
    def run(self, run_input: Dict[str, Any]) -> Dict[str, Any]:
        """Execute agent workflow with given inputs.
        
        Args:
            run_input: Runtime parameters (e.g., topic, depth, timeframe)
            
        Returns:
            Dictionary containing execution results, QA metrics, and artifacts
        """
        self.ctx = ToolContext(self.spec, run_input, self.out_dir)
        
        try:
            return self._execute_workflow()
        except Exception as e:
            self.ctx.log(f"Execution failed: {e}", level="ERROR")
            raise
    
    def _execute_workflow(self) -> Dict[str, Any]:
        """Internal workflow execution logic."""
        topic = self.ctx.run_input.get("topic", "topic")
        
        # Log execution plan
        plan_hints = self.spec.plan.get("hints", ["collect", "process", "output"])
        self.ctx.log(f"Starting workflow: {' → '.join(plan_hints)}")
        
        # Step 1: Search and collect
        if "search" not in self.spec.tools:
            self.ctx.log("search: tool not enabled", level="WARNING")
            results = []
        else:
            results = self.tools["search"](self.ctx, query=topic)
        
        # Apply domain filters
        results = self._apply_domain_filters(results)
        
        # Step 2: Fetch content
        if "fetch" not in self.spec.tools:
            self.ctx.log("fetch: tool not enabled", level="WARNING")
            docs = []
        else:
            urls = [r["url"] for r in results[:5]]
            docs = self.tools["fetch"](self.ctx, urls=urls)
        
        # Step 3: Summarize/cluster
        if "summarize" not in self.spec.tools:
            self.ctx.log("summarize: tool not enabled", level="WARNING")
            themes = {}
        else:
            themes = self.tools["summarize"](self.ctx, docs=docs)
        
        # Step 4: Create bullet points
        bullets = self._create_bullets(themes, results)
        
        # Step 5: Write output
        outfile = None
        if "write" in self.spec.tools:
            outfile = self.tools["write"](self.ctx, bullets)
        else:
            self.ctx.log("write: tool not enabled, skipping output")
        
        # Compute QA metrics
        output_texts = [b["body"] for b in bullets]
        qa_metrics = QA.compute_metrics(self.spec, self.ctx, output_texts)
        
        self.ctx.log(f"Workflow complete in {self.ctx.elapsed():.2f}s")
        
        return {
            "outfile": str(outfile) if outfile else None,
            "bullets": bullets,
            "qa": qa_metrics,
            "logs": self.ctx.logs,
            "sources": self.ctx.sources,
            "elapsed_seconds": round(self.ctx.elapsed(), 2),
        }
    
    def _apply_domain_filters(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Apply whitelist/blacklist domain filters to search results."""
        whitelist = self.spec.constraints.get("domains_whitelist", [])
        blacklist = self.spec.constraints.get("domains_blacklist", [])
        
        if not whitelist and not blacklist:
            return results
        
        filtered = []
        for result in results:
            domain = result.get("domain", "").lower()
            
            # Check blacklist first
            if blacklist:
                if any(bl.replace("*.", "").lower() in domain for bl in blacklist):
                    continue
            
            # Check whitelist
            if whitelist:
                if not any(wl.replace("*.", "").lower() in domain for wl in whitelist):
                    continue
            
            filtered.append(result)
        
        if filtered:
            self.ctx.log(f"filter: {len(filtered)}/{len(results)} results passed domain filters")
            return filtered
        else:
            self.ctx.log("filter: no results passed filters, using unfiltered", level="WARNING")
            return results
    
    def _create_bullets(self, themes: Dict[str, List[str]], 
                       results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Create structured bullet points from themes."""
        bullets: List[Dict[str, Any]] = []
        
        # Deterministic randomness for confidence scores
        topic = self.ctx.run_input.get("topic", "topic")
        rnd = random.Random(hash(topic) & 0xFFFFFFFF)
        
        show_confidence = "show_confidence" in self.spec.display_addons
        include_sources = "include_sources" in self.spec.display_addons
        
        for theme, items in themes.items():
            body = f"Key aspects: {', '.join(items)}."
            confidence = 0.85 + rnd.random() * 0.10  # 0.85-0.95 range
            
            bullet = {
                "title": theme,
                "body": body,
            }
            
            if show_confidence:
                bullet["confidence"] = confidence
            
            if include_sources and results:
                bullet["sources"] = results[:2]  # Top 2 sources per bullet
            
            bullets.append(bullet)
        
        return bullets


# ============================================================================
# I/O and Configuration
# ============================================================================

EXAMPLE_FORM = {
    "Name": "researcher",
    "Goal": "Find reliable sources on topic X and produce bullet points of problems and solutions.",
    "Focus": "reliability, modern solutions",
    "Display": {
        "Type": "bullet-points",
        "Add-ons": "include_sources, show_confidence, group_by_theme"
    },
    "Inputs": "topic, depth, timeframe",
    "Tools": "search, fetch, summarize, write",
    "Specifics": {
        "Steps": "collect → cluster → deduplicate → bulletize",
        "Domains": {
            "whitelist": "*.gov, *.edu, reputable news"
        }
    },
    "Approvals": "",
    "Memory": "both",
    "Complexity": "medium",
    "Autonomy": "medium",
    "Notes": "Prefer sources published in the last 24 months."
}

EXAMPLE_RUN_INPUT = {
    "topic": "AI safety and elections",
    "depth": "brief",
    "timeframe": "24 months"
}


def load_form(path: Path) -> AgentForm:
    """Load and parse agent form from JSON file.
    
    If file doesn't exist, creates example form.
    
    Args:
        path: Path to form JSON file
        
    Returns:
        Parsed AgentForm object
        
    Raises:
        json.JSONDecodeError: If file contains invalid JSON
        IOError: If file cannot be read
    """
    if not path.exists():
        logger.info(f"Form not found, creating example at {path}")
        path.write_text(
            json.dumps(EXAMPLE_FORM, indent=2, ensure_ascii=False),
            encoding="utf-8"
        )
    
    try:
        raw = json.loads(path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON in {path}: {e}")
        raise
    except IOError as e:
        logger.error(f"Cannot read {path}: {e}")
        raise
    
    # Map user-friendly keys to AgentForm fields (case-insensitive)
    def get_field(field_name: str) -> Any:
        """Case-insensitive field lookup."""
        for key, value in raw.items():
            if key.lower() == field_name.lower():
                return value
        return None
    
    return AgentForm(
        name=get_field("Name"),
        goal=get_field("Goal"),
        focus=get_field("Focus"),
        display=get_field("Display"),
        inputs=get_field("Inputs"),
        tools=get_field("Tools"),
        specifics=get_field("Specifics"),
        approvals=get_field("Approvals"),
        memory=get_field("Memory"),
        complexity=get_field("Complexity"),
        autonomy=get_field("Autonomy"),
        notes=get_field("Notes"),
    )


def load_run_input(path: Path) -> Dict[str, Any]:
    """Load run input parameters from JSON file.
    
    If file doesn't exist, creates example input.
    
    Args:
        path: Path to input JSON file
        
    Returns:
        Dictionary of runtime parameters
        
    Raises:
        json.JSONDecodeError: If file contains invalid JSON
        IOError: If file cannot be read
    """
    if not path.exists():
        logger.info(f"Input not found, creating example at {path}")
        path.write_text(
            json.dumps(EXAMPLE_RUN_INPUT, indent=2, ensure_ascii=False),
            encoding="utf-8"
        )
    
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON in {path}: {e}")
        raise
    except IOError as e:
        logger.error(f"Cannot read {path}: {e}")
        raise


def save_artifacts(result: Dict[str, Any], out_dir: Path) -> None:
    """Save execution artifacts to output directory.
    
    Args:
        result: Execution result dictionary
        out_dir: Output directory path
    """
    artifacts = {
        "qa.json": result.get("qa", {}),
        "bullets.json": result.get("bullets", []),
        "sources.json": result.get("sources", []),
    }
    
    # Save JSON artifacts
    for filename, data in artifacts.items():
        try:
            path = out_dir / filename
            path.write_text(
                json.dumps(data, indent=2, ensure_ascii=False),
                encoding="utf-8"
            )
            logger.info(f"Saved {path}")
        except IOError as e:
            logger.error(f"Failed to save {filename}: {e}")
    
    # Save logs
    try:
        logs_path = out_dir / "logs.txt"
        logs_path.write_text(
            "\n".join(result.get("logs", [])),
            encoding="utf-8"
        )
        logger.info(f"Saved {logs_path}")
    except IOError as e:
        logger.error(f"Failed to save logs: {e}")


# ============================================================================
# Main Entry Point
# ============================================================================

def main(argv: Optional[List[str]] = None) -> int:
    """Main execution function.
    
    Args:
        argv: Command line arguments (None = use sys.argv)
        
    Returns:
        Exit code (0 = success, non-zero = error)
    """
    parser = argparse.ArgumentParser(
        description="Agent-of-Agents scaffold - Execute configured agents with mock tools",
        epilog=f"Version {__version__}"
    )
    parser.add_argument(
        "--form",
        default="agent_form.json",
        help="Path to agent intake form JSON (default: agent_form.json)"
    )
    parser.add_argument(
        "--input",
        default="run_input.json",
        help="Path to run input JSON (default: run_input.json)"
    )
    parser.add_argument(
        "--out",
        default="out",
        help="Output directory (default: out)"
    )
    parser.add_argument(
        "--version",
        action="version",
        version=f"%(prog)s {__version__}"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose logging"
    )
    
    args = parser.parse_args(argv)
    
    # Configure logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    form_path = Path(args.form)
    input_path = Path(args.input)
    out_dir = Path(args.out)
    
    # Create output directory
    try:
        out_dir.mkdir(parents=True, exist_ok=True)
    except OSError as e:
        logger.error(f"Cannot create output directory {out_dir}: {e}")
        return 1
    
    # Load and validate form
    try:
        logger.info(f"Loading form from {form_path}")
        form = load_form(form_path)
    except Exception as e:
        logger.error(f"Failed to load form: {e}")
        return 1
    
    logger.info("Validating form...")
    is_valid, errors = Validator.validate_form(form)
    
    if not is_valid:
        logger.error("Validation failed with the following errors:")
        for i, error in enumerate(errors, 1):
            logger.error(f"  {i}. {error}")
        return 2
    
    logger.info("✓ Validation passed")
    
    # Compile form to spec
    try:
        logger.info("Compiling agent specification...")
        spec = Compiler.compile(form)
    except Exception as e:
        logger.error(f"Compilation failed: {e}")
        return 1
    
    # Save compiled spec
    try:
        compiled_path = out_dir / "compiled_spec.json"
        compiled_path.write_text(
            json.dumps(dataclasses.asdict(spec), indent=2, ensure_ascii=False),
            encoding="utf-8"
        )
        logger.info(f"✓ Compiled spec saved to {compiled_path}")
    except IOError as e:
        logger.warning(f"Could not save compiled spec: {e}")
    
    # Load run input
    try:
        logger.info(f"Loading run input from {input_path}")
        run_input = load_run_input(input_path)
    except Exception as e:
        logger.error(f"Failed to load run input: {e}")
        return 1
    
    # Execute agent
    try:
        logger.info("Starting agent execution...")
        orchestrator = Orchestrator(spec, out_dir)
        result = orchestrator.run(run_input)
    except Exception as e:
        logger.error(f"Execution failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    # Save artifacts
    try:
        save_artifacts(result, out_dir)
    except Exception as e:
        logger.warning(f"Failed to save some artifacts: {e}")
    
    # Print summary
    logger.info("=" * 60)
    logger.info("Execution complete!")
    logger.info(f"Elapsed time: {result.get('elapsed_seconds', 0):.2f}s")
    logger.info("")
    logger.info("Generated artifacts:")
    if result.get("outfile"):
        logger.info(f"  ✓ {result['outfile']}")
    logger.info(f"  ✓ {out_dir / 'qa.json'}")
    logger.info(f"  ✓ {out_dir / 'logs.txt'}")
    logger.info(f"  ✓ {out_dir / 'bullets.json'}")
    logger.info(f"  ✓ {out_dir / 'sources.json'}")
    logger.info("")
    logger.info("QA Metrics:")
    qa = result.get("qa", {})
    for metric, value in qa.items():
        logger.info(f"  {metric}: {value}")
    logger.info("=" * 60)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
